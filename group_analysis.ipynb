{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group analysis (Gaussian and DoG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linescanning import (\n",
    "    prf,\n",
    "    optimal,\n",
    "    pycortex,\n",
    "    plotting,\n",
    "    fitting,\n",
    "    glm,\n",
    "    utils\n",
    ")\n",
    "from prfpy import model, rf, timecourse\n",
    "from prfpy.stimulus import PRFStimulus2D\n",
    "from prfpy.stimulus import *\n",
    "from prfpy.fit import *\n",
    "from prfpy.model import *\n",
    "from prfpy import timecourse\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import cortex\n",
    "opj = os.path.join\n",
    "opd = os.path.dirname\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from statsmodels.stats.weightstats import DescrStatsW, ttost_paired\n",
    "from statsmodels.stats.descriptivestats import Description\n",
    "from seaborn import regression\n",
    "import statsmodels.api as sm   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profile and add. measures (fwhmax, fwatmin, half max, min profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Profile and add. measures (fwhmax, fwatmin, half max, min profile)\n",
    "class Add_measures():\n",
    "    \n",
    "    'To obtain profile, fwhmax, halfmax, fwatmin, min profile'\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        data = None, \n",
    "        model = None,\n",
    "        ecc = True,\n",
    "        #params=None,  \n",
    "        # roi = None,\n",
    "        normalize_RFs = False\n",
    "        ):\n",
    "            \n",
    "        self.data = data\n",
    "        self.model = model\n",
    "        #self.params = params\n",
    "        #self.roi = roi\n",
    "        self.normalize_RFs = normalize_RFs\n",
    "        self.ecc = ecc\n",
    "        \n",
    "        \n",
    "        self.fwhmax = []\n",
    "        self.fwhmin = []\n",
    "        self.halfmax = []\n",
    "        self.minprofile = [] \n",
    "        self.profile_df = pd.DataFrame()\n",
    "        \n",
    "        if self.ecc:\n",
    "            self.data = self.data[(self.data.ecc > 0.5) & (self.data.ecc < 4.51)] # not including those vooxels with higher ecc - not even for the profile\n",
    "        \n",
    "        for index in range(len(self.data)):    \n",
    "            fwhmaxx, halfmaxx, fwhminn,  minprofilee, profile , self.x = self.fwhmax_fwatmin(model = self.model, \n",
    "                                                                                   params = self.data.iloc[index], \n",
    "                                                                                   normalize_RFs = self.normalize_RFs)  # fwhmax, half_max, [], [], list(profile.T), x\n",
    "            \n",
    "            self.fwhmax.append(float(fwhmaxx))\n",
    "            self.halfmax.append(float(halfmaxx))\n",
    "            self.profile_df[index] = profile\n",
    "            \n",
    "            if self.model == 'dog':\n",
    "                self.minprofile.append(float(minprofilee))\n",
    "                self.fwhmin.append(float(fwhminn))\n",
    "            \n",
    "        \n",
    "        self.data['fwhmax']= self.fwhmax\n",
    "        self.data['halfmax'] = self.halfmax\n",
    "\n",
    "        if self.model == 'dog':\n",
    "            self.data['fwmin'] = self.fwhmin\n",
    "            self.data['min_profile'] = self.minprofile\n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "    def fwhmax_fwatmin(self, model, params, normalize_RFs):\n",
    "    \n",
    "        model = model.lower()\n",
    "        x = np.linspace(-20,20,1000).astype('float32')\n",
    "\n",
    "        prf = params.prf_ampl * np.exp(-0.5*x[...,np.newaxis]**2 / params.prf_size**2) #params[...,3] * np.exp(-0.5*x[...,np.newaxis]**2 / params[...,2]**2)\n",
    "        vol_prf =  2*np.pi*params.prf_size**2\n",
    "\n",
    "        if 'dog' in model or 'norm' in model:\n",
    "            srf = params.surr_ampl * np.exp(-0.5*x[...,np.newaxis]**2 / params.surr_size**2)\n",
    "            vol_srf = 2*np.pi*params.surr_size**2\n",
    "\n",
    "        if normalize_RFs==True:\n",
    "\n",
    "            if model == 'gauss':\n",
    "                profile =  prf / vol_prf\n",
    "\n",
    "            elif model =='dog':\n",
    "                profile = prf / vol_prf - \\\n",
    "                        srf / vol_srf\n",
    "        else:\n",
    "            if model == 'gauss':\n",
    "                profile = prf\n",
    "            elif model =='dog':\n",
    "                profile = prf - srf\n",
    "\n",
    "        half_max = np.max(profile, axis=0)/2\n",
    "        \n",
    "        fwhmax = np.abs(2*x[np.argmin(np.abs(profile-half_max), axis=0)])\n",
    "\n",
    "\n",
    "        if 'dog' in model or 'norm' in model:\n",
    "\n",
    "            min_profile = np.min(profile, axis=0)\n",
    "            \n",
    "            fwatmin = np.abs(2*x[np.argmin(np.abs(profile-min_profile), axis=0)])\n",
    "\n",
    "            return fwhmax, fwatmin, half_max, min_profile, list(profile.T), x\n",
    "        else:\n",
    "            return fwhmax, half_max, [], [], list(profile.T), x\n",
    "    \n",
    "class Add_measures_2d():\n",
    "    \n",
    "    'To obtain profile, fwhmax, halfmax, fwatmin, min profile'\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        data = None, \n",
    "        model = None,\n",
    "        ecc = True,\n",
    "        #params=None,  \n",
    "        # roi = None,\n",
    "        normalize_RFs = False\n",
    "        ):\n",
    "            \n",
    "        self.data = data\n",
    "        self.model = model\n",
    "        #self.params = params\n",
    "        #self.roi = roi\n",
    "        self.normalize_RFs = normalize_RFs\n",
    "        self.ecc = ecc\n",
    "        \n",
    "        self.design, self.prf_stim = self.load_dm()\n",
    "        \n",
    "        self.fwhmax = []\n",
    "        self.fwhmin = []\n",
    "        self.halfmax = []\n",
    "        self.minprofile = [] \n",
    "        self.profilex_df = pd.DataFrame()\n",
    "        self.profiley_df = pd.DataFrame()\n",
    "        \n",
    "        if self.ecc:\n",
    "            self.data = self.data[(self.data.ecc > 0.5) & (self.data.ecc < 4.51)] # not including those vooxels with higher ecc - not even for the profile\n",
    "        \n",
    "        for index in range(len(self.data)):    \n",
    "            fwhmaxx, halfmaxx, fwhminn,  minprofilee, profilex, profiley , self.x = self.fwhmax_fwatmin(model = self.model, \n",
    "                                                                                   params = self.data.iloc[index], \n",
    "                                                                                   normalize_RFs = self.normalize_RFs, prf_stim = self.prf_stim)  # fwhmax, half_max, [], [], list(profile.T), x\n",
    "            \n",
    "            self.fwhmax.append(float(fwhmaxx))\n",
    "            self.halfmax.append(float(halfmaxx))\n",
    "            self.profilex_df[index] = profilex\n",
    "            self.profiley_df[index] = profiley\n",
    "            \n",
    "            if self.model == 'dog':\n",
    "                self.minprofile.append(float(minprofilee))\n",
    "                self.fwhmin.append(float(fwhminn))\n",
    "            \n",
    "        \n",
    "        self.data['fwhmax']= self.fwhmax\n",
    "        self.data['halfmax'] = self.halfmax\n",
    "\n",
    "        if self.model == 'dog':\n",
    "            self.data['fwmin'] = self.fwhmin\n",
    "            self.data['min_profile'] = self.minprofile\n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "    def fwhmax_fwatmin(self, model, params, normalize_RFs, prf_stim):\n",
    "    \n",
    "        model = model.lower()\n",
    "        x = np.linspace(-20,20,1000).astype('float32')\n",
    "\n",
    "        prf = params.prf_ampl * np.exp(-0.5*x[...,np.newaxis]**2 / params.prf_size**2) #params[...,3] * np.exp(-0.5*x[...,np.newaxis]**2 / params[...,2]**2)\n",
    "        vol_prf =  2*np.pi*params.prf_size**2\n",
    "\n",
    "        if 'dog' in model or 'norm' in model:\n",
    "            srf = params.surr_ampl * np.exp(-0.5*x[...,np.newaxis]**2 / params.surr_size**2)\n",
    "            vol_srf = 2*np.pi*params.surr_size**2\n",
    "\n",
    "        if normalize_RFs==True:\n",
    "\n",
    "            if model == 'gauss':\n",
    "                profile =  prf / vol_prf\n",
    "\n",
    "            elif model =='dog':\n",
    "                profile = prf / vol_prf - \\\n",
    "                        srf / vol_srf\n",
    "        else:\n",
    "            if model == 'gauss':\n",
    "                profile = prf\n",
    "            elif model =='dog':\n",
    "                profile = prf - srf\n",
    "\n",
    "        half_max = np.max(profile, axis=0)/2\n",
    "        \n",
    "        fwhmax = np.abs(2*x[np.argmin(np.abs(profile-half_max), axis=0)])\n",
    "\n",
    "        if 'dog' in model or 'norm' in model:\n",
    "\n",
    "            min_profile = np.min(profile, axis=0)\n",
    "            \n",
    "            fwatmin = np.abs(2*x[np.argmin(np.abs(profile-min_profile), axis=0)])\n",
    "            \n",
    "            profilex = self.make_prf(model = model, stim = prf_stim, params = params)[0]\n",
    "            profiley = self.make_prf(model = model, stim = prf_stim, params = params)[1]\n",
    "\n",
    "            return fwhmax, fwatmin, half_max, min_profile, list(profilex.T), list(profiley.T), x\n",
    "        else:\n",
    "            profilex = self.make_prf(model = model, stim = prf_stim, params = params)[0]\n",
    "            profiley = self.make_prf(model = model, stim = prf_stim, params = params)[1]\n",
    "            return fwhmax, half_max, [], [], list(profilex.T), list(profiley.T), x\n",
    "        \n",
    "    \n",
    "    def make_prf(self, model, stim, params, mu_x=0, mu_y=0, size=None, resize_pix=None, **kwargs):\n",
    "        \"\"\"make_prf\n",
    "\n",
    "        Create an instantiation of a pRF using the parameters obtained during fitting.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        prf_object: prfpy.stimulus.PRFStimulus2D\n",
    "            representation the pRF in visual space\n",
    "        mu_x: float\n",
    "            x-component of pRF. Leave default if you want to create size/response functions\n",
    "        mu_y: float\n",
    "            y-component of pRF. Leave default if you want to create size/response functions\n",
    "        size: float\n",
    "            size of pRF, optional\n",
    "        resize_pix: int\n",
    "            resolution of pRF to resample to. For instance, if you've used a low-resolution design matrix, but you'd like a prettier image, you can set `resize` to something higher than the original (54 >> 270, for example). By default not used.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        numpy.ndarray\n",
    "            meshgrid containing Gaussian characteristics of the pRF. Can be plotted with :func:`linescanning.plotting.LazyPRF`\n",
    "\n",
    "        prf = np.rot90(rf.gauss2D_iso_cart(\n",
    "            x=prf_object.x_coordinates[..., np.newaxis],\n",
    "            y=prf_object.y_coordinates[..., np.newaxis],\n",
    "            mu=(mu_x, mu_y),\n",
    "            sigma=size,\n",
    "            normalize_RFs=False).T, axes=(1, 2))\n",
    "        \"\"\"\n",
    "        \n",
    "        prf = params.prf_ampl[...,np.newaxis,np.newaxis]*np.rot90(\n",
    "            rf.gauss2D_iso_cart(\n",
    "                x=stim.x_coordinates[...,np.newaxis],\n",
    "                y=stim.y_coordinates[...,np.newaxis],\n",
    "                mu=(params.x, params.y),\n",
    "                sigma=params.prf_size,\n",
    "                normalize_RFs=False).T, axes=(1,2))\n",
    "        \n",
    "        if model == 'dog':\n",
    "        \n",
    "            prf -= params.surr_ampl[...,np.newaxis,np.newaxis]*np.rot90(\n",
    "                    rf.gauss2D_iso_cart(\n",
    "                        x=stim.x_coordinates[...,np.newaxis],\n",
    "                        y=stim.y_coordinates[...,np.newaxis],\n",
    "                        mu=(params.x, params.y),\n",
    "                        sigma=params.surr_size,\n",
    "                        normalize_RFs=False).T, axes=(1,2))\n",
    "            \n",
    "        #print(x, y)\n",
    "\n",
    "        # spatially smooth for visualization\n",
    "        if isinstance(resize_pix, int):\n",
    "            prf_squeezed = np.squeeze(prf, axis=0)\n",
    "            prf = utils.resample2d(prf_squeezed, resize_pix)\n",
    "        \n",
    "        # save a bunch of problems if returned array is 2D\n",
    "        if prf.ndim > 2:\n",
    "            prf = np.squeeze(prf,axis=0)\n",
    "            \n",
    "        return prf\n",
    "    \n",
    "    \n",
    "    def load_dm(cut_volumes = True):\n",
    "\n",
    "        #### load design matrix ###\n",
    "        screen_size_cm =39.3\n",
    "        screen_distance_cm=210\n",
    "        grid_nr = 20\n",
    "        TR= 1.5\n",
    "        \n",
    "        if cut_volumes:\n",
    "            n_volumes = 5\n",
    "        else:\n",
    "            n_volumes = 0\n",
    "\n",
    "        design = prf.read_par_file(opj(opd(opd(prf.__file__)), '/data1/projects/Meman1/projects/pilot/code', 'design_task-2R.mat'))\n",
    "\n",
    "        prf_stim = PRFStimulus2D(screen_size_cm = screen_size_cm,\n",
    "                                    screen_distance_cm = screen_distance_cm,\n",
    "                                    design_matrix = design[:, :, n_volumes:], # remove first 5 volumes\n",
    "                                    TR = TR,\n",
    "                                    task_names ='2R')\n",
    "        \n",
    "        return design, prf_stim\n",
    "    \n",
    "def load_dm(cut_volumes = True):\n",
    "\n",
    "    #### load design matrix ###\n",
    "    screen_size_cm =39.3\n",
    "    screen_distance_cm=210\n",
    "    grid_nr = 20\n",
    "    TR= 1.5\n",
    "    \n",
    "    if cut_volumes:\n",
    "        n_volumes = 5\n",
    "    else:\n",
    "        n_volumes = 0\n",
    "\n",
    "    design = prf.read_par_file(opj(opd(opd(prf.__file__)), '/data1/projects/Meman1/projects/pilot/code', 'design_task-2R.mat'))\n",
    "\n",
    "    prf_stim = PRFStimulus2D(screen_size_cm = screen_size_cm,\n",
    "                                screen_distance_cm = screen_distance_cm,\n",
    "                                design_matrix = design[:, :, n_volumes:], # remove first 5 volumes\n",
    "                                TR = TR,\n",
    "                                task_names ='2R')\n",
    "    \n",
    "    return design, prf_stim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load parameters ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load parameters ROIs\n",
    "def load_params_roi(sub, roi, model, cv):\n",
    "    \n",
    "    if roi != 'v1':\n",
    "        roi = roi.upper()\n",
    "    if cv:\n",
    "        add = '_train-test'\n",
    "    else:\n",
    "        add = ''\n",
    "    \n",
    "    ### SESSION 2\n",
    "    prf_fn_s2 = opj(\"/data1/projects/Meman1/projects/pilot/derivatives/prf\",f'sub-{sub}', f\"ses-2{add}\", f\"sub-{sub}_ses-2_task-2R_roi-{roi}_model-{model}_stage-iter_desc-prf_params.pkl\") #f\"sub-{sub}_ses-2_task-2R_roi-{roi}_model-{model}_stage-iter_desc-prf_params.pkl\"\n",
    "    \n",
    "    pars_s2 = prf.Parameters(prf.read_par_file(prf_fn_s2), model=model).to_df() #gettig prf parameters \n",
    "    #data_s2 = prf.Parameters(prf.read_par_file(prf_fn_s2), model=model).to_df() #gettig prf parameters \n",
    "    data_s2 = pars_s2.loc[pars_s2.r2 != 0] \n",
    "    #data_s2 = pars_s2.loc[(pars_s2.ecc>0) & (pars_s2.ecc<=5)] # selecting voxels which are true for V1 (whole roi) and ecc higher than 0\n",
    "    \n",
    "    data_s2['roi'] = roi\n",
    "    \n",
    "    data_s2['session'] = 2\n",
    "    \n",
    "    if cv:\n",
    "        cvrsq_s2 = pd.read_csv(f'/data1/projects/Meman1/projects/pilot/derivatives/prf/sub-{sub}/ses-2{add}/rsqs_{roi}_2_sub{sub}.csv')\n",
    "        cvrsq_s2.index = data_s2.index\n",
    "    else:\n",
    "        cvrsq_s2 = []\n",
    "    \n",
    "    ### SESSION 3\n",
    "    \n",
    "    prf_fn_s3 = opj(\"/data1/projects/Meman1/projects/pilot/derivatives/prf\",f'sub-{sub}', f\"ses-3{add}\", f\"sub-{sub}_ses-3_task-2R_roi-{roi}_model-{model}_stage-iter_desc-prf_params.pkl\") #f\"sub-{sub}_ses-2_task-2R_roi-{roi}_model-{model}_stage-iter_desc-prf_params.pkl\"\n",
    "    \n",
    "    pars_s3 = prf.Parameters(prf.read_par_file(prf_fn_s3), model=model).to_df() #gettig prf parameters \n",
    "    #data_s3 = prf.Parameters(prf.read_par_file(prf_fn_s3), model=model).to_df() #gettig prf parameters \n",
    "            \n",
    "    data_s3 = pars_s3.loc[pars_s3.r2 != 0] \n",
    "    # data_s3 = pars_s3.loc[(pars_s3.ecc>0) & (pars_s3.ecc<=5)] # selecting voxels which are true for V1 (whole roi) and ecc higher than 0\n",
    "\n",
    "    data_s3['roi'] = roi\n",
    "    \n",
    "    data_s3['session'] = 3\n",
    "    \n",
    "    if cv:\n",
    "        cvrsq_s3 = pd.read_csv(f'/data1/projects/Meman1/projects/pilot/derivatives/prf/sub-{sub}/ses-3{add}/rsqs_{roi}_3_sub{sub}.csv')\n",
    "        cvrsq_s3.index = data_s3.index\n",
    "    else:\n",
    "        cvrsq_s3 = []\n",
    "    \n",
    "    if sub in ['003', '004', '007', '012', '016']: # meman session 2\n",
    "        \n",
    "        data_s2['ses'] = 'memantine'\n",
    "        \n",
    "        data_s3['ses'] = 'placebo'\n",
    "        \n",
    "    else:  ## meman in session 3     \n",
    "        \n",
    "        data_s3['ses'] = 'memantine'\n",
    "        \n",
    "        data_s2['ses'] = 'placebo'\n",
    "\n",
    "    data = pd.concat([data_s2, data_s3])\n",
    "    \n",
    "    data['subject'] = sub\n",
    "    \n",
    "    if cv:\n",
    "        data['rsq_test'] = pd.concat([cvrsq_s2['tc_rsq_test'], cvrsq_s3['tc_rsq_test']])\n",
    "    \n",
    "    dataf = Add_measures(data = data, model = model, normalize_RFs = False, ecc = True)\n",
    "    \n",
    "    \n",
    "    #dataf = fwhmax_fwhmin_allvoxels(data = data, model = 'dog', normalize_RFs = False) # calculate fwhmax, fwhmin\n",
    "    \n",
    "    #datae = dataf.data[dataf.data.ecc < 5.5]\n",
    "    \n",
    "    #datae = dataf[dataf.ecc<5.5]\n",
    "    \n",
    "    dataf.data.to_csv(f'/data1/projects/Meman1/projects/pilot/derivatives/prf/sub-{sub}/sub-{sub}_roi-{roi}_{model}_parameters.csv')\n",
    "    \n",
    "    # dataf.data contains the whole dataset\n",
    "    # dataf.profile contains the profile for each voxel\n",
    "    \n",
    "    return dataf, cvrsq_s2, cvrsq_s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load all sbjects class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllSubjParams():\n",
    "    \n",
    "    '''\n",
    "    Class to fit specific dog parameter across eccentricity bins for all subjects, using a weighted r2 mean or not (w = True /False) and plot it\n",
    "        \n",
    "        y: can be 'yfit'(fitted bins to intercept and slope, estimated using all data) or 'y' (actual data binned)\n",
    "        \n",
    "        w: use the weighted mean for r2 or just the mean of the bin . if W = false : median instead of mean is used and no weighted by R2\n",
    "        \n",
    "        testprint : show posthoc test parameters or not - it is a ttest using FDR correction\n",
    "                    ## FDR-corrected post hocs with Cohen's D effect size self.posthoc = pingouin.pairwise_tests(\n",
    "                    ## uses the parametric ttest() function. If False, use pingouin.wilcoxon() or pingouin.mwu() for paired or unpaired samples, respectively.\n",
    "                        if 0.01 < p_val < 0.05:txt = \"*\"\n",
    "                        elif 0.001 < p_val < 0.01:txt = \"**\"\n",
    "                        elif p_val < 0.001:txt = \"***\"\n",
    "                    \n",
    "    It returns a barplot with error of all parameters across both sessions and the points values (bin values). It shows whether differences between sessions for specific parameter are significant or not.\n",
    "                            \n",
    "    '''\n",
    "        \n",
    "    def __init__(\n",
    "        self, \n",
    "        data=None, \n",
    "        par=None, \n",
    "        #roi=None, \n",
    "        nbins=None, \n",
    "        y=None, \n",
    "        w=None, \n",
    "        figsize=(4,8),\n",
    "        testprint=False, \n",
    "        axs=None,\n",
    "        **kwargs):\n",
    "    \n",
    "    \n",
    "        ## unify all subjects fitting parameter ###\n",
    "        \n",
    "        self.data = data\n",
    "        self.par = par\n",
    "        self.rois = data.roi.unique()\n",
    "        self.nbins = nbins\n",
    "        self.y = y\n",
    "        self.w = w\n",
    "        self.testprint = testprint\n",
    "        self.figsize = figsize\n",
    "        self.axs = axs\n",
    "        self.subjects = data.subject.unique()\n",
    "\n",
    "        self.regdf = pd.DataFrame()\n",
    "        \n",
    "        for sub in self.subjects:\n",
    "            for roi in self.rois:\n",
    "                #_, _, self.regr_df, self.x = self.fit_param_bins(\n",
    "                \n",
    "                #self.regr_df, self.x = self.fit_param_binss(\n",
    "                _, _, self.regr_df, self.x = self.fit_param_bins(\n",
    "                    sub, \n",
    "                    roi, \n",
    "                    self.par, \n",
    "                    self.data[self.data.subject == sub], \n",
    "                    self.nbins, \n",
    "                    self.w)  \n",
    "                \n",
    "                self.regdf = pd.concat([self.regdf, self.regr_df])\n",
    "                \n",
    "            self.df = self.regdf[self.regdf.parameter == self.par]\n",
    "        \n",
    "        \n",
    "        if not \"ses_short\" in list(self.df.columns):\n",
    "            ses_short = self.df[\"ses\"].values.copy()\n",
    "            ses_short = np.array([i[:3] for i in ses_short])\n",
    "\n",
    "            self.df[\"ses_short\"] = ses_short\n",
    "        \n",
    " \n",
    "        \n",
    "    def fit_param_bins(self, sub, roi, par, data, nbins, w):\n",
    "        \n",
    "        '''\n",
    "        Function to fit specific dog parameter across eccentricity bins, using a weighted r2 mean or not (w = True /False)\n",
    "        \n",
    "        if W = false : median instead of mean is used and no weighted by R2\n",
    "        '''\n",
    "        \n",
    "        binpar ='ecc'#'std_prf_ecc'#'std_size'\n",
    "        b = np.linspace(0.5, 4.5, nbins)\n",
    "        x = np.asarray([(b[i]+b[i+1])/2 for i in range(len(b)-1)])\n",
    "        \n",
    "        ### data for each session\n",
    "        s0 = data[(data.roi == roi) & (data.ses == 'placebo')] # data for placebo session(data.subject == sub) & \n",
    "        s1 = data[(data.roi == roi) &  (data.ses == 'memantine')] # data for memantine session(data.subject == sub)\n",
    "\n",
    "    \n",
    "        ### fitting it to a linear function - estimating intercept and slope ###\n",
    "        p0 = np.polyfit(s0[binpar], s0[par], 1) # placebo - fitting it to a linear function - finding intercept and slope\n",
    "        p1 = np.polyfit(s1[binpar], s1[par], 1) # memantine, full = True   - fitting it to a linear function - finding intercept and slope\n",
    "        \n",
    "        S0 = sm.add_constant(s0[binpar])\n",
    "        S1 = sm.add_constant(s1[binpar])\n",
    "        wp0 = sm.WLS(s0[par], S0, weights=s0['r2']).fit()\n",
    "        wp1 = sm.WLS(s1[par], S1, weights=s1['r2']).fit()\n",
    "        #print(wp0.fittedvalues)\n",
    "        #print(wp0.params)\n",
    "        \n",
    "        wyfit0 = np.polyval(wp0.params, x)\n",
    "        wyfit1 = np.polyval(wp1.params, x)\n",
    "\n",
    "        ### calculating predicted values ###\n",
    "        yfit0 = np.polyval(p0, x)\n",
    "        yfit1 = np.polyval(p1, x)\n",
    "        \n",
    "        \n",
    "        ### get mean values for each bin ###\n",
    "        y0, std0 = self.return_binned(s0, \n",
    "                                        s0[par], \n",
    "                                        s0[binpar],\n",
    "                                        w, \n",
    "                                        list(b)) # mean value for the bin - real data\n",
    "\n",
    "        y1, std1 = self.return_binned(s1, \n",
    "                                      s1[par], \n",
    "                                      s1[binpar], \n",
    "                                      w, \n",
    "                                      list(b)) # mean value for each bin - real data\n",
    "        \n",
    "        yfit0 = np.polyval(p0, x)\n",
    "        yfit1 = np.polyval(p1, x)\n",
    "        \n",
    "        ### create df with fitted values ###\n",
    "        \n",
    "        # df = pd.DataFrame({\"fit\": np.concatenate(tmp_list), \"ix\": np.concatenate(tmp_ix), \"ses\": np.concatenate(tmp_name), \"subject\": np.concatenate(tmp_subj)})\n",
    "\n",
    "        regr_df = pd.DataFrame()\n",
    "        std1 = np.array(std1)\n",
    "        std0 = np.array(std0)\n",
    "        regr_df['yfit'] = np.hstack([yfit0, yfit1])\n",
    "        regr_df['wyfit'] = np.hstack([wyfit0, wyfit1])\n",
    "        regr_df['y'] = np.hstack([y0, y1])\n",
    "        regr_df['std'] = np.hstack([std0, std1]) #std of the bin mea    \n",
    "        #regr_df['ci_l'] = np.hstack([ci_l0, ci_l1]) #std of the bin mean\n",
    "        regr_df['ses'] = np.hstack([np.repeat('placebo', nbins-1), np.repeat('memantine', nbins-1)])\n",
    "        regr_df['subject'] = np.repeat(sub, nbins*2-2)\n",
    "        regr_df['intercept'] = np.hstack([np.repeat(p0[0], nbins-1), np.repeat(p1[0], nbins-1)])    \n",
    "        regr_df['slope'] = np.hstack([np.repeat(p0[1], nbins-1), np.repeat(p1[1], nbins-1)])\n",
    "        regr_df['wintercept'] = np.hstack([np.repeat(wp0.params[0], nbins-1), np.repeat(wp1.params[0], nbins-1)])    \n",
    "        regr_df['wslope'] = np.hstack([np.repeat(wp0.params[1], nbins-1), np.repeat(wp1.params[1], nbins-1)])\n",
    "        regr_df['diff_fit'] = np.hstack([yfit1-yfit0, yfit1-yfit0]) # memantine - placebo (putting it twice to adjust in df\n",
    "        regr_df['diff_y'] = np.hstack([y1-y0, y1-y0]) # memantine - placebo (putting it twice to adjust in df\n",
    "        regr_df['diff_std'] = np.hstack([np.abs(std1-std0), np.abs(std1-std0)]) # memantine - placebo (putting it twice to adjust in df\n",
    "        regr_df['roi'] = roi\n",
    "        regr_df['parameter'] = par\n",
    "\n",
    "        std_resid0 = np.std(y0-yfit0)\n",
    "        std_resid1 = np.std(y1-yfit1)\n",
    "\n",
    "        return p0, p1, regr_df, x\n",
    "    \n",
    "    \n",
    "    def fit_param_binss(self, roi, par, data, nbins, w):\n",
    "        \n",
    "        '''\n",
    "        Function to fit specific dog parameter across eccentricity bins, using a weighted r2 mean or not (w = True /False)\n",
    "        \n",
    "        We are obtaining the CI of each fit using seaborn regression package\n",
    "        \n",
    "        No weighted mean is used here (we only need the fitted differences)\n",
    "        \n",
    "        The grid is performed tiny differently (it has 20 values instead of 18 as above)\n",
    "        '''\n",
    "        \n",
    "        binpar ='ecc'#'std_prf_ecc'#'std_size'\n",
    "        \n",
    "        b = np.linspace(0.5, 4.5, nbins)\n",
    "        \n",
    "        ### data for each session\n",
    "        s0 = data[(data.roi == roi) & (data.ses == 'placebo')] # data for placebo session(data.subject == sub) & \n",
    "        s1 = data[(data.roi == roi)  & (data.ses == 'memantine')] # data for memantine session& (data.subject == sub)\n",
    "\n",
    "    \n",
    "        ### fitting it to a linear function - estimating intercept and slope ###\n",
    "        \n",
    "        #p0 = np.polyfit(s0[binpar], s0[par], 1) # placebo - fitting it to a linear function - finding intercept and slope\n",
    "\n",
    "        #p1 = np.polyfit(s1[binpar], s1[par], 1) # memantine, full = True   - fitting it to a linear function - finding intercept and slope\n",
    "\n",
    "        reg_pla = regression._RegressionPlotter(data = s0, x = 'ecc',  y=par,  x_bins = nbins\n",
    "                                                , x_ci = None)\n",
    "        \n",
    "        reg_mem = regression._RegressionPlotter(data = s1, x = 'ecc',  y=par,  x_bins = nbins\n",
    "                                                , x_ci = None)\n",
    "\n",
    "        grid, yhat_pla, err_bands_pla = reg_pla.fit_regression(grid = b) #  return grid, yhat, err_bands\n",
    "        grid, yhat_mem, err_bands_mem = reg_mem.fit_regression(grid = b) #  return grid, yhat, err_bands\n",
    "\n",
    "        regr_df = pd.DataFrame()\n",
    "        regr_df['yfit'] = np.hstack([yhat_pla, yhat_mem])\n",
    "        regr_df['ci_l'] = np.hstack([err_bands_pla[0], err_bands_mem[0]]) #std of the bin mean\n",
    "        regr_df['ci_u'] = np.hstack([err_bands_pla[1], err_bands_mem[1]]) #std of the bin mean\n",
    "        regr_df['ses'] = np.hstack([np.repeat('placebo', nbins), np.repeat('memantine', nbins)])\n",
    "        #regr_df['subject'] = np.repeat(sub, nbins*2)\n",
    "        regr_df['diff_fit'] = np.hstack([yhat_mem-yhat_pla, yhat_mem-yhat_pla]) # memantine - placebo (putting it twice to adjust in df)\n",
    "        regr_df['diff_cil'] = np.hstack([err_bands_mem[0]-err_bands_pla[0], err_bands_mem[0]-err_bands_pla[0]])  # memantine - placebo (putting it twice to adjust in df)\n",
    "        regr_df['diff_ciu'] = np.hstack([err_bands_mem[1]-err_bands_pla[1], err_bands_mem[1]-err_bands_pla[1]])  # memantine - placebo (putting it twice to adjust in df)\n",
    "        regr_df['roi'] = roi\n",
    "        regr_df['parameter'] = par\n",
    "    \n",
    "        return regr_df, grid\n",
    "    \n",
    "    \n",
    "    def return_binned(self, data, data_par, binon, w, bins=None, start=None, stop=None, binsize=None, nbins=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        Bins data according to provided bins. Can provide either a list of bins,\n",
    "        or start, stop and bin sizes or nbins.\n",
    "\n",
    "        Equivalent to: scipy.stats.binned_statistic()\n",
    "\n",
    "        data (1D array) : values to be averaged within a bin\n",
    "        binon (1D array) : independent values to bin-on (separate from values being averaged)\n",
    "        bins (list) : list of bins\n",
    "        start (float) :\n",
    "        stop (float) :\n",
    "        binsize (float) :\n",
    "        nbins (int) :\n",
    "\n",
    "        Returns\n",
    "        binned_data (arr)\n",
    "        \n",
    "        if W = false : median instead of mean is used and no weighted by R2\n",
    "        using the wighted mean by r2, the fitting is much closer to it\n",
    "        \n",
    "        median is much closer to the fitting as well - avoid outliers having too much influence \n",
    "        \n",
    "        fitting is performed to all data - no bins\n",
    "        \"\"\"\n",
    "\n",
    "        #data=np.asarray(data_par)\n",
    "        binon=np.asarray(binon)\n",
    "\n",
    "        assert data_par.ndim,'data must be 1D'\n",
    "\n",
    "        if not bins and binsize:\n",
    "            assert start and stop, 'provid list of bins or start and stop values'\n",
    "            bins=np.arange(start,stop,binsize)\n",
    "        elif not bins and nbins:\n",
    "            assert start and stop, 'provid list of bins or start and stop values'\n",
    "            bins=np.linspace(start,stop,nbins)\n",
    "        else:\n",
    "            assert bins, 'provide bins'\n",
    "\n",
    "        binned = [data_par[(binon >= bins[i]) & (binon<bins[i+1])].median() for i in range(len(bins)-1)] # data=data[par]\n",
    "        #print(binned)\n",
    "        std = [data_par[(binon >= bins[i]) & (binon<bins[i+1])].std() for i in range(len(bins)-1)] # not very useful if using the median but good to check outliers?\n",
    "        \n",
    "        \n",
    "        \n",
    "        if w: # calculate the mean of each bin (made by eccentricity size) by r2 weighted - those voxels with higher explainable variance will have more weight in the mean\n",
    "            binned = [DescrStatsW(data = data_par[(binon >= bins[i]) & (binon<bins[i+1])], weights = data[(binon >= bins[i]) & (binon<bins[i+1])].r2).mean for i in range(len(bins)-1)]\n",
    "            std = [DescrStatsW(data = data_par[(binon >= bins[i]) & (binon<bins[i+1])], weights = data[(binon >= bins[i]) & (binon<bins[i+1])].r2).std_mean for i in range(len(bins)-1)]\n",
    "        \n",
    "        return np.asarray(binned), std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted mean profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wmean_profile(param_sub, roi):\n",
    "    \n",
    "    '''param_sub: is the object created that returns the data with all parameters and the profile\n",
    "    \n",
    "    It computes weighted mean by r2 of the profiles of the subject (point by point) and not the parameter which can be affected by outliers/etc\n",
    "    '''\n",
    "\n",
    "    profile_sub = param_sub.profile_df.T[0].apply(pd.Series)  # 10000 values - \n",
    "    #convert each point calculated into a column in df\n",
    "\n",
    "    profile_sub.index = param_sub.data.index # get same indexes to locate\n",
    "    \n",
    "    profile_sub['r2'] = param_sub.data.r2\n",
    "\n",
    "    pla_profile = []\n",
    "    mem_profile = []\n",
    "    std_mem = []\n",
    "    std_pla = []\n",
    "\n",
    "\n",
    "    for point in range(1000):\n",
    "        print(profile_sub[param_sub.data.ses == 'placebo'][point])\n",
    "        print(profile_sub.r2) \n",
    "                                                \n",
    "        std_pla.append(DescrStatsW(data = profile_sub[param_sub.data.ses == 'placebo'][point], \n",
    "                            weights = profile_sub[param_sub.data.ses == 'placebo'].r2).std_mean )# standard deviation of the weighted mean\n",
    "                            \n",
    "        std_mem.append(DescrStatsW(data = profile_sub[param_sub.data.ses == 'memantine'][point], \n",
    "                            weights = profile_sub[param_sub.data.ses == 'memantine'].r2).std_mean) # standard deviation of the weighted mean\n",
    "        \n",
    "        pla_profile.append( DescrStatsW(data = profile_sub[param_sub.data.ses == 'placebo'][point], \n",
    "                            weights = profile_sub[param_sub.data.ses == 'placebo'].r2).mean )\n",
    "        \n",
    "        mem_profile.append( DescrStatsW(data = profile_sub[param_sub.data.ses == 'memantine'][point], \n",
    "                            weights = profile_sub[param_sub.data.ses == 'memantine'].r2).mean )\n",
    "    \n",
    "    profile = pd.DataFrame({'ses': np.array(param_sub.data.ses.unique()), \n",
    "                            'subject': np.hstack([param_sub.data.subject.unique(), param_sub.data.subject.unique()]),\n",
    "                            'roi': [roi, roi]})\n",
    "    \n",
    "    profile = pd.concat([profile, pd.DataFrame([pla_profile, mem_profile])], axis = 1)\n",
    "    \n",
    "    return profile, std_pla, std_mem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rois = ['v1', 'v2', 'v3']\n",
    "subjects = [ '001','005','007', '008', '010', '012'] \n",
    "model = 'dog'\n",
    "cv = False\n",
    "\n",
    "parameters_subs = pd.DataFrame()\n",
    "profiles_subs = pd.DataFrame()\n",
    "\n",
    "for sub in subjects:\n",
    "    print(sub)\n",
    "    \n",
    "    for roi in rois:\n",
    "        print(roi)\n",
    "        \n",
    "        param_sub, _, _ = load_params_roi(sub, roi, model, cv)\n",
    "    \n",
    "        parameters_subs = pd.concat([parameters_subs, param_sub.data], axis = 0) #parameters\n",
    "        \n",
    "        profile, std_pla, std_mem = wmean_profile(param_sub, roi) # weighted mean of profiles (not parameters)    \n",
    "        profiles_subs = pd.concat([profiles_subs, profile], axis = 0)\n",
    "\n",
    "        # plotting.LazyPlot(\n",
    "        #     ts = [profile.loc[1][3:],\n",
    "        #           profile.loc[0][3:]],\n",
    "        #     xx = param_sub.x, # I could reduce it\n",
    "        #     color = ['indianred', 'black'],\n",
    "        #     labels = ['memantine', 'placebo'],\n",
    "        #     title = f'Mean profile sub-{sub} ({roi.upper()})',\n",
    "        #     x_label = 'x (deg of visual field)',\n",
    "        #     y_label = f'amplitude - {model} model',\n",
    "        #     figsize = [10,10],\n",
    "        #     #markers = [None, '.'],\n",
    "        #     line_width  = [3,2],\n",
    "        #     font_size = 30,\n",
    "        #     #save_as = f\"/data1/projects/Meman1/projects/pilot/code/results_images/medianprofile_sub-{sub}_roi-{roi}.jpg\"\n",
    "        #     save_as = f\"/data1/projects/Meman1/projects/pilot/code/results_images/wmeanprofile_sub-{sub}_roi-{roi}_{model}.jpg\"\n",
    "            \n",
    "        # )\n",
    "        \n",
    "print(np.unique(parameters_subs.subject))\n",
    "\n",
    "parameters_subs.to_csv(f'all_subjects_parameters_{model}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_parameters = pd.read_csv('/data1/projects/Meman1/projects/pilot/code/def_code/all_subjects_parameters_dog.csv')\n",
    "\n",
    "gauss_parameters = pd.read_csv('/data1/projects/Meman1/projects/pilot/code/def_code/all_subjects_parameters_dog.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig,axs = plt.subplots(nrows=3, figsize=(8,16), constrained_layout=True, sharey = True)\n",
    "\n",
    "# ax1 = fig.add_subplot(gs[0])\n",
    "# ax2 = fig.add_subplot(gs[1])\n",
    "# ax3 = fig.add_subplot(gs[2])\n",
    "\n",
    "for i,roi in enumerate(profiles_subs.roi.unique()):\n",
    "    profile = profiles_subs[profiles_subs.roi == roi]\n",
    "    \n",
    "    mean_mem = profile[profile.ses == 'memantine']\n",
    "    mean_pla = profile[profile.ses == 'placebo']  \n",
    "    \n",
    "    # df_ks = pd.DataFrame()\n",
    "    # df_ks['Income'] = np.sort(df['Income'].unique())\n",
    "    # df_ks['F_control'] = df_ks['Income'].apply(lambda x: np.mean(income_c<=x))\n",
    "    # df_ks['F_treatment'] = df_ks['Income'].apply(lambda x: np.mean(income_t<=x))\n",
    "    # df_ks.head() \n",
    "    \n",
    "    plotting.LazyPlot(\n",
    "                ts = [mean_mem.mean()[1:],\n",
    "                    mean_pla.mean()[1:]],\n",
    "                xx = param_sub.x, \n",
    "                color = ['indianred', 'black'],\n",
    "                labels = ['memantine', 'placebo'],\n",
    "                title = f'\\n {roi.upper()}',\n",
    "                #x_label = 'x ',\n",
    "                y_label = f'amplitude',\n",
    "                figsize = [4,15],\n",
    "                axs = axs[i],\n",
    "                add_hline = 0,\n",
    "                #markers = ['.', '.'],\n",
    "                line_width  = [2,1],\n",
    "                font_size = 20,\n",
    "                label_size = 17,\n",
    "                x_lim = [-15,15],\n",
    "                #y_lim = [-0.007,0.02],\n",
    "                #save_as = f\"/data1/projects/Meman1/projects/pilot/code/results_images/medianprofile_sub-{sub}_roi-{roi}.jpg\"\n",
    "                #save_as = f\"/data1/projects/Meman1/projects/pilot/code/results_images/wmeanprofile_allsub_roi-{roi}_{model}.jpg\"\n",
    "                \n",
    "            )\n",
    "fig.suptitle('Weighted mean profile - DoG model',y = 1.05, fontsize = 20)\n",
    "#fig.supylabel(f'amplitude' , fontsize = 13)\n",
    "fig.supxlabel('x (deg. visual field)', fontsize = 20, y = -.05, x = .55) #\n",
    "#plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DISTRIBUTION OF MAIN PARAMETERS ACROSS V1, SESSION, SUBJECT - eccentricity max set below 5.5####\n",
    "\n",
    "parameters = ['fwhmax', 'fwmin', 'halfmax', 'ecc','r2' ]#'prf_size', 'prf_ampl','surr_size', 'surr_ampl']#,]#'min_profile',\n",
    "\n",
    "g = plt.figure(figsize = [45,15])\n",
    "axes = g.subplots(1,len(parameters))\n",
    "\n",
    "for i, par in enumerate(parameters):\n",
    "   \n",
    "    sns.set(font_scale = 1.5)\n",
    "    \n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    sns.boxplot(y= par, x= \"subject\", hue='ses', data=parameters_subs,  ax = axes[i], palette =['#cccc', 'indianred']).set_title(par)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific distribution for each subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SPECIFIC PARAMETER DISTRIBUTION FOR EACH SUBJECT ####\n",
    "\n",
    "parameters = parameters_subs.columns.drop(['roi', 'session','subject', 'polar', 'ecc', 'r2', 'bold_bsl', 'x', 'y', 'ses'])\n",
    "\n",
    "param = 'min_profile'\n",
    "\n",
    "g = plt.figure(figsize = [25,10])\n",
    "axes = g.subplots(1,5)\n",
    "\n",
    "for subject, ax in zip(np.unique(parameters_subs.subject), axes.flatten()):\n",
    "    \n",
    "    sns.violinplot(y=param, x= \"ses\",  data=parameters_subs[parameters_subs.subject ==subject], ax=ax).set_title(f'{subject} - {param}')\n",
    "    #axes.set_ylim([0,0.02])\n",
    "    sns.despine(ax=ax)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset tatistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stats = parameters_subs.groupby(['subject', 'ses','roi']).describe().T\n",
    "\n",
    "data_stats.to_csv(f'df_statistics_transpose_{model}.csv')\n",
    "\n",
    "data_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effects of memantine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FWHM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each subject fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'dog'\n",
    "\n",
    "parameters_subs = pd.read_csv(f'all_subjects_parameters_{model}.csv')\n",
    "\n",
    "data = parameters_subs\n",
    "nbins =15\n",
    "y = 'yfit'\n",
    "w = True\n",
    "par = 'fwmin'\n",
    "\n",
    "regdf = AllSubjParams(\n",
    "        data=data, \n",
    "        par=par, \n",
    "        #roi=roi, \n",
    "        nbins=nbins, \n",
    "        y=y, \n",
    "        w=w, \n",
    "        testprint=False,\n",
    "        fancy=True,\n",
    "        error=\"sem\",\n",
    "        sns_offset=4,\n",
    "        #axs=axs[ix],\n",
    "        fancy_denom=6,\n",
    "        label_size=16,\n",
    "        font_size=20) # points of plot are the bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rois = regdf.df.roi.unique()\n",
    "pla = regdf.df[regdf.df.ses=='placebo']\n",
    "mem = regdf.df[regdf.df.ses=='memantine']\n",
    "#fig,axs = plt.subplots(ncols=1, figsize=(19,8), constrained_layout=True, sharey = True)\n",
    "parameter = 'fwmin'\n",
    "\n",
    "sns.set_context('paper',font_scale=2.5)\n",
    "\n",
    "a = sns.lmplot(data =  parameters_subs[(parameters_subs.ecc>=0.5) & (parameters_subs.ecc<=4.5)], x='ecc', \n",
    "                   y = parameter, row = 'subject', col= 'roi', hue = 'ses',  x_ci = None, palette =['grey','indianred' ], scatter = False, height= 8)\n",
    "\n",
    "# axs.errorbar(regdf.x, mem[mem.roi== roi].y, yerr=mem[mem.roi== rois[0]]['std'], fmt=\"o\", c='indianred')\n",
    "#     axs[i].errorbar(regdf.x, pla[pla.roi== roi].y, yerr=pla[pla.roi== roi]['std'], fmt=\"o\", c='#ccc')\n",
    "#plt.suptitle('FWHM - DoG model \\n \\n \\n', fontsize = 20)\n",
    "\n",
    "for ix, sub in enumerate(parameters_subs.subject.unique()):\n",
    "    for i,roi in enumerate(list(parameters_subs.roi.unique())):\n",
    "        a.axes[ix][i].errorbar(regdf.x, pla[(pla.roi== roi) & (pla.subject== sub)].y, yerr=pla[(pla.roi== roi) & (pla.subject== sub)]['std'], fmt=\"o\", c='grey', markersize = 10, capsize = 6)\n",
    "        a.axes[ix][i].errorbar(regdf.x, mem[(mem.roi== roi) & (mem.subject== sub)].y, yerr=mem[(mem.roi== roi) & (mem.subject== sub)]['std'], fmt=\"o\", c='indianred', markersize = 10, capsize = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Across all subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'gauss'\n",
    "\n",
    "parameters_subs = pd.read_csv(f'all_subjects_parameters_{model}.csv')\n",
    "\n",
    "data = parameters_subs\n",
    "nbins =15\n",
    "y = 'yfit'\n",
    "w = True\n",
    "par = 'fwhmax'\n",
    "\n",
    "regdf = AllSubjParams(\n",
    "        data=data, \n",
    "        par=par, \n",
    "        #roi=roi, \n",
    "        nbins=nbins, \n",
    "        y=y, \n",
    "        w=w, \n",
    "        testprint=False,\n",
    "        fancy=True,\n",
    "        error=\"sem\",\n",
    "        sns_offset=4,\n",
    "        #axs=axs[ix],\n",
    "        fancy_denom=6,\n",
    "        label_size=16,\n",
    "        font_size=20) # points of plot are the bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rois = regdf.df.roi.unique()\n",
    "pla = regdf.df[regdf.df.ses=='placebo']\n",
    "mem = regdf.df[regdf.df.ses=='memantine']\n",
    "#fig,axs = plt.subplots(ncols=1, figsize=(19,8), constrained_layout=True, sharey = True)\n",
    "parameter = 'fwhmax'\n",
    "\n",
    "sns.set_context('paper',font_scale=2.5)\n",
    "\n",
    "a = sns.lmplot(data =  parameters_subs[(parameters_subs.ecc>=0.5) & (parameters_subs.ecc<=4.5)], x='ecc', \n",
    "                   y = parameter, col= 'roi', hue = 'ses',  x_ci = None, palette =['grey','indianred' ], scatter = False, height= 8)\n",
    "\n",
    "# axs.errorbar(regdf.x, mem[mem.roi== roi].y, yerr=mem[mem.roi== rois[0]]['std'], fmt=\"o\", c='indianred')\n",
    "#     axs[i].errorbar(regdf.x, pla[pla.roi== roi].y, yerr=pla[pla.roi== roi]['std'], fmt=\"o\", c='#ccc')\n",
    "#plt.suptitle('FWHM - DoG model \\n \\n \\n', fontsize = 20)\n",
    "\n",
    "\n",
    "for i,roi in enumerate(list(parameters_subs.roi.unique())):\n",
    "    a.axes[0][i].errorbar(regdf.x, [pla[pla.roi == roi]['y'].loc[i].mean() for i in range(14)], yerr=[pla[pla.roi == roi]['std'].loc[i].mean() for i in range(14)], fmt=\"o\", c='grey', markersize = 10, capsize = 6)\n",
    "    a.axes[0][i].errorbar(regdf.x, [mem[mem.roi == roi]['y'].loc[i].mean() for i in range(14,28)], yerr=[mem[mem.roi == roi]['std'].loc[i].mean() for i in range(14,28)], fmt=\"o\", c='indianred', markersize = 10, capsize = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Difference of fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rois = regdf.df.roi.unique()\n",
    "pla = regdf.df[regdf.df.ses=='placebo']\n",
    "mem = regdf.df[regdf.df.ses=='memantine']\n",
    "\n",
    "plotting.LazyPlot(\n",
    "   ts = [[pla[pla.roi == rois[0]]['diff_fit'].loc[i].mean() for i in range(14)] ,\n",
    "         [pla[pla.roi == rois[1]]['diff_fit'].loc[i].mean() for i in range(14)] ,\n",
    "         [pla[pla.roi == rois[2]]['diff_fit'].loc[i].mean() for i in range(14)]\n",
    "         ],\n",
    "   xx = regdf.x,\n",
    "   x_label='eccentricity',\n",
    "    y_label=f'diff. FWMIN',\n",
    "    title = f'FWMIN (memantine - placebo)',\n",
    "    labels = [ 'V1', \n",
    "              'V2', \n",
    "              'V3',\n",
    "                ],\n",
    "    cmap = 'inferno',\n",
    "    #color = ['red','orange', 'yellow'],\n",
    "    # line_width = [0.5, 2, 0.5, \n",
    "    #               0.5, 2, 0.5\n",
    "    #               ,0.5, 2, 0.5,\n",
    "    #               0.5, 2, 0.5, \n",
    "    #                0.5, 2, 0.5,\n",
    "    #               0.5, 2, 0.5],\n",
    "#     markers = [, \n",
    "#                '.', \n",
    "#                '.'],\n",
    "    #alpha = [0.2, 1, 0.2],\n",
    "    line_width = 5,\n",
    "    figsize = [10,6],\n",
    "    add_hline = 0,\n",
    "    label_size = 20,\n",
    "    save_as = f\"/data1/projects/Meman1/projects/pilot/code/results_images/difffwhmax_allsub_{model}.jpg\"\n",
    ")\n",
    "plt.tight_layout"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
